{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting String data to integer, and normalizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Breast_Cancer_dataset.csv')\n",
    "for col in df.columns:\n",
    "    if not np.issubdtype(df[col].dtype, np.number):\n",
    "        unique_values = len(df[col].unique())\n",
    "        df[col] = pd.Categorical(df[col]).codes + 1\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "df.to_csv('python_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    df = pd.read_csv('python_preprocessed.csv')\n",
    "    X = df.drop(columns=['Status'])\n",
    "    y = df['Status']\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k=10) \n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "\n",
    "    df_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    df_selected['Status'] = y.values\n",
    "\n",
    "\n",
    "    training_set = df_selected.sample(frac=0.8)\n",
    "    test_set = df_selected.drop(training_set.index)\n",
    "\n",
    "    X_train = training_set.drop(columns=['Status'])\n",
    "    y_train = training_set['Status']\n",
    "    X_test = test_set.drop(columns=['Status'])\n",
    "    y_test = test_set['Status']\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# test\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Cleaning and Missing Value Replacement**:  \n",
    "   Missing values were addressed by replacing them with the average value of each respective feature.\n",
    "\n",
    "2. **Normalization**:  \n",
    "   Normalization was performed using the formula:\n",
    "   $\\text{val} = \\frac{\\text{val} - \\text{min}}{\\text{max} - \\text{min}}$\n",
    "   \n",
    "\n",
    "3. **Balancing the Dataset**:  \n",
    "   The dataset contains a significantly higher number of patients marked as \"alive\" compared to those marked as \"dead.\" For certain algorithms (specially KNN which we manually coded), we adjusted the training set by rescaling the proportion of \"alive\" patients to achieve better balance.\n",
    "\n",
    "4. **Feature Selection**:  \n",
    "   Feature selection was conducted using the `f_classif` function from the `scikit-learn` package. \\\n",
    "   This method selects the top \\(k\\) features based on the ANOVA F-value, calculated as $\\frac{out-group variance}{in-group variance} $. The top 10 features were selected for the final model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is just finding the k-nearest neighbors and taking a majority vote, and it is implemented this from scratch\n",
    "The main hyperparameter is the number of neighbors to check for voting, which is set to 10 in this case.\n",
    "\n",
    "KNN is very easy to implement and understand, but it is computationally expensive and not very efficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "# Distance between two rows of data\n",
    "def distance(row1, row2):\n",
    "    dist = 0\n",
    "    for i in range(len(row1) - 1):\n",
    "        dist += (row1.iloc[i] - row2.iloc[i]) ** 2\n",
    "    return dist ** 0.5\n",
    "\n",
    "\n",
    "def knn( point, x_train, y_train, k = 10):\n",
    "    distances = []\n",
    "    for i in range(len(x_train)):\n",
    "        dist = distance(point, x_train.iloc[i])\n",
    "        distances.append((y_train.iloc[i], dist))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = [x[0] for x in distances[:k]]\n",
    "    return max(set(neighbors), key=neighbors.count)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "total_correct = 0\n",
    "for i in range(len(X_test)):\n",
    "    prediction = knn(X_test.iloc[i], X_train , y_train)\n",
    "    if y_test.iloc[i] == prediction:\n",
    "        total_correct += 1\n",
    "\n",
    "print(f'Accuracy: {total_correct / len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic classifier based on applying Bayes' theorem with the assumption that features are independent of each other. \n",
    "The main hyperparameter is the smoothing parameter, which is set to 1e-9 in this case.\n",
    "\n",
    "Naive Bayes is very efficient and works well small or Large datasets, but it is sensitive to feature independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5 Decision Tree is a recursive algorithm that splits the data based on the the most useful feature.\n",
    "The main hyperparameter is the \"Criteria\" which is the method used to split the data, and also the maximum_depth and min_samples_split describing the shape of the tree. \n",
    "This model is easy to visualize and understand and performs feature selection automatically, but it is prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4.5 Decision Tree\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is essentially an randomly picked subset of decision trees. This way it reduces the overfitting that decision tree can have.\n",
    "The main hyperparameters are the same as the decision tree.\n",
    "\n",
    "This model is accurate and works with large dataset very well, but can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is a mixture of tree models and build them on top of each other.\n",
    "The main hyperparameters is learning_rate which is the step size at each iteration, and n_estimators which is the number of boosting stages to perform.\n",
    "\n",
    "This model is very accurate, it is the algorithm that performed the best in our experiment, but it can sometimes cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Gradient Boosting\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network is the state of the art algorithm that consits of layers of nodes, it can capture complex patterns in the data.\n",
    "The main hyperparameters are the number of layers, the number of nodes in each layer.\n",
    "\n",
    "This model is very accurate and can capture complex patterns, but it is computationally expensive and can be hard to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning on NN\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = split_data()\n",
    "\n",
    "nn = MLPClassifier()\n",
    "hidden_layer_sizes = [(100,), (200,), (300,) , (400,), (500,)]\n",
    "max_iter = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Display the performance\n",
    "all_results = []\n",
    "\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    for iter in max_iter:\n",
    "        nn.set_params(hidden_layer_sizes=hidden_layer_size, max_iter=iter)\n",
    "        nn.fit(X_train, y_train)\n",
    "        y_pred = nn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Hidden Layer Size: {hidden_layer_size}, Max Iter: {iter}, Accuracy: {accuracy}\")\n",
    "        all_results.append((hidden_layer_size, iter, accuracy))\n",
    "\n",
    "# find best hyperparameters and the accuracy\n",
    "best_hyperparameters = max(all_results, key=lambda x: x[2])\n",
    "print(f\"Best Hyperparameters: {best_hyperparameters[0]}, {best_hyperparameters[1]}, Accuracy: {best_hyperparameters[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Performance\n",
    "| Hidden Layer Size | Max Iterations | Accuracy             |\n",
    "|-------------------|----------------|----------------------|\n",
    "| (100,)            | 100            | 0.8907               |\n",
    "| (100,)            | 200            | 0.8957               |\n",
    "| (100,)            | 300            | 0.8957               |\n",
    "| (100,)            | 400            | 0.8932               |\n",
    "| (100,)            | 500            | **0.8994**           |\n",
    "| (200,)            | 100            | 0.8994               |\n",
    "| (200,)            | 200            | 0.8919               |\n",
    "| (200,)            | 300            | 0.8919               |\n",
    "| (200,)            | 400            | 0.8981               |\n",
    "| (200,)            | 500            | 0.8932               |\n",
    "| (300,)            | 100            | 0.8981               |\n",
    "| (300,)            | 200            | 0.8944               |\n",
    "| (300,)            | 300            | 0.8981               |\n",
    "| (300,)            | 400            | 0.8994               |\n",
    "| (300,)            | 500            | 0.8957               |\n",
    "\n",
    "### Best Hyperparameters\n",
    " **Hidden Layer Size:** (100,) **Max Iterations:** 500  **Accuracy:** 0.8994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning on Random Forest\n",
    "\n",
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "max_depth = [10, 50, 100, 200, 300]\n",
    "results = []\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    for depth in max_depth:\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.set_params(n_estimators=n_estimator, max_depth=depth)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"n_estimators: {n_estimator}, max_depth: {depth}, Accuracy: {accuracy}\")\n",
    "        results.append((n_estimator, depth, accuracy))\n",
    "\n",
    "best_hyperparameters = max(results, key=lambda x: x[2])\n",
    "print(f\"Best Hyperparameters: {best_hyperparameters[0]}, {best_hyperparameters[1]}, Accuracy: {best_hyperparameters[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Parameter Tuning Performance\n",
    "| n_estimators | max_depth | Accuracy             |\n",
    "|--------------|-----------|----------------------|\n",
    "| 200          | 10        | 0.9081               |\n",
    "| 200          | 50        | 0.9043               |\n",
    "| 200          | 100       | 0.8957               |\n",
    "| 200          | 200       | 0.9019               |\n",
    "| 200          | 300       | 0.9056               |\n",
    "| 300          | 10        | **0.9093**           |\n",
    "| 300          | 50        | 0.9006               |\n",
    "| 300          | 100       | 0.8981               |\n",
    "| 300          | 200       | 0.8994               |\n",
    "| 300          | 300       | 0.9019               |\n",
    "| 400          | 10        | 0.9056               |\n",
    "| 400          | 50        | 0.9019               |\n",
    "| 400          | 100       | 0.9031               |\n",
    "| 400          | 200       | 0.9019               |\n",
    "| 400          | 300       | 0.9006               |\n",
    "\n",
    "### Best Hyperparameters\n",
    "**n_estimators:** 300 **max_depth:** 10 **Accuracy:** 0.9093\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "|Method        | KNN   | Naive Bayes | C4.5 Decision Tree | Random Forest | Gradient Boosting | Neural Network| \n",
    "| ---          | ---   | ---         | ---                | ---           | ---               | --- |\n",
    "|No Selection  | 0.746 | 0.806       | 0.816              | 0.891         | 0.896             |  0.861 |\n",
    "|ANOVA F-value | 0.846 | 0.831       | 0.847              | 0.875        | 0.909             | 0.902 |\n",
    "\n",
    "\n",
    "With feature selection, the accuracy of most models are improved, the feature we selected are \"T Stage\", \"N Stage\", \"6th Stage\", \"Grade\", \"A Stage\", \"Tumor Size\", \"Estrogen Status\", \"Progesterone Status\", \"Reginol Node Positive\", \"Survival Months\". And the features deemed not useful are Age, Race, Marital Status, Differentiate, Regional Node Examined.\n",
    "\n",
    "The best model is Gradient Boosting followed by Neural network, but most models have over 80% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
